{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이트판 톡커들의 선택(일간) 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '20130101'   # 시작 날짜 입력\n",
    "end_date   = '20130102'   # 종료 날짜 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 글 링크 모으기 (하루 당 100개의 글 존재)\n",
    "def Get_url():\n",
    "    url_list = []\n",
    "    for date in range(int(start_date), int(end_date)+1):\n",
    "        for page in [1, 2]:\n",
    "            url = f'https://pann.nate.com/talk/ranking/d?stdt={date}&page={page}'\n",
    "            response = requests.get(url)\n",
    "            time.sleep(0.3)\n",
    "            if response.status_code != requests.codes.ok: # 접속 실패\n",
    "                print(\"접속 실패\")\n",
    "                continue\n",
    "\n",
    "            html = BeautifulSoup(response.text, 'html.parser')\n",
    "            tags = html.select('div.cntList dt a')\n",
    "\n",
    "            for tag in tags:\n",
    "                url_list.append('https://pann.nate.com'+tag.attrs['href'])\n",
    "    \n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 약간의 전처리\n",
    "def preprocessing(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\t', '')\n",
    "    if '이미지확대보기' in text: # 이미지 첨부된 경우\n",
    "        text = text.replace('이미지확대보기', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 본문 및 댓글 크롤링\n",
    "def Crawling(url_list):\n",
    "    content_list  = []  # 본문 리스트\n",
    "    reply_list    = []  # 댓글 리스트\n",
    "    subreply_list = []  # 대댓글 리스트\n",
    "\n",
    "    \n",
    "    for url in url_list:\n",
    "        # 페이지 접속\n",
    "        pann_id = url[-9:]\n",
    "        response = requests.get(url)\n",
    "        time.sleep(1)\n",
    "        if response.status_code != requests.codes.ok: # 접속 실패\n",
    "            print(f\"{url} 접속 실패\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        # 본문\n",
    "        html = BeautifulSoup(response.text, 'html.parser')\n",
    "        tags = html.select('div#contentArea')\n",
    "        for tag in tags:\n",
    "            content = tag.text\n",
    "            content = content.replace('\\n', ' ')\n",
    "            content = content.replace('\\t', '')\n",
    "            content = content.replace('\\xa0', '')\n",
    "            if '이미지확대보기' in content: # 이미지 첨부된 경우\n",
    "                content = content.replace('이미지확대보기', '')\n",
    "            content_list.append(content)\n",
    "    \n",
    "    \n",
    "        # 댓글\n",
    "        reply_page = 1\n",
    "        bef_reply_text = None\n",
    "        while True:\n",
    "            # 웹 버전에서는 댓글 페이지가 동적이어서 모바일 버전으로 가져옴\n",
    "            # 그럼 아예 모바일 버전으로 하면 되지 않나요? -> 모바일 버전에서는 댓글을 보려면 댓글 버튼을 눌러야해서 selenium 필요 ^^;\n",
    "            reply_url = f'https://m.pann.nate.com/talk/reply/view?pann_id={pann_id}&page={reply_page}'\n",
    "            response = requests.get(reply_url)\n",
    "            time.sleep(0.5)\n",
    "            if response.status_code != requests.codes.ok: # 접속 실패\n",
    "                print(f\"{reply_url} 접속 실패\")\n",
    "                continue\n",
    "\n",
    "\n",
    "            # 현재 댓글 페이지에 나온 댓글들의 id 저장\n",
    "            reply_id_list = [] \n",
    "            html = BeautifulSoup(response.text, 'html.parser')\n",
    "            cur_reply_href = html.select('div#listDiv dl dt a')\n",
    "            for href_tag in cur_reply_href:\n",
    "                href = href_tag.attrs['href']\n",
    "                reply_id = href[-9:]\n",
    "                reply_id_list.append(reply_id)\n",
    "\n",
    "            # 댓글 내용 저장\n",
    "            cur_reply_text = []\n",
    "            cur_reply_text = html.select('div#listDiv dd.userText')\n",
    "            if bef_reply_text == cur_reply_text:  # 댓글 마지막 페이지\n",
    "                break\n",
    "            bef_reply_text = cur_reply_text\n",
    "\n",
    "            for idx, text_tag in enumerate(cur_reply_text):\n",
    "                reply = preprocessing(text_tag.text)\n",
    "                reply_list.append(reply)\n",
    "\n",
    "                # 대댓글\n",
    "                reply_id = reply_id_list[idx]\n",
    "                subreply_page = 1\n",
    "                bef_subreply_text = None\n",
    "                subreply_temp_list = []\n",
    "                while True:\n",
    "                    subreply_url = f'https://m.pann.nate.com/talk/reply/subReply?pann_id={pann_id}&prts_reply_id={reply_id}&page={subreply_page}'\n",
    "                    response = requests.get(subreply_url)\n",
    "                    time.sleep(0.5)\n",
    "                    if response.status_code != requests.codes.ok: # 접속 실패\n",
    "                        print(f\"{subreply_url} 접속 실패\")\n",
    "                        continue\n",
    "\n",
    "                    html = BeautifulSoup(response.text, 'html.parser')\n",
    "                    cur_subreply_text = html.select('div#listDiv dd.userText em') \n",
    "                    if bef_subreply_text == cur_subreply_text:  # 대댓글 마지막 페이지\n",
    "                        break\n",
    "                    bef_subreply_text = cur_subreply_text\n",
    "\n",
    "                    for text_tag in cur_subreply_text:\n",
    "                        subreply = preprocessing(text_tag.text)\n",
    "                        subreply_temp_list.append(subreply)\n",
    "\n",
    "                    subreply_page += 1\n",
    "\n",
    "                subreply_list.append(subreply_temp_list)\n",
    "\n",
    "            reply_page += 1\n",
    "    \n",
    "    \n",
    "    return content_list, reply_list, subreply_list\n",
    "\n",
    "# reply_list와 subreply_list는 인덱스에 따라 대응함\n",
    "# subreply_list[0]이 ['a','b','c']라면, reply_list[0] 댓글에 3개의 대댓글이 달린 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # csv 파일로 저장 (수정 필요)\n",
    "# def Save_csv(content_list, reply_list):\n",
    "#     df = pd.DataFrame(content_list+reply_list, columns=[\"text\"])\n",
    "\n",
    "#     # UnicodeEncodeError 에러 발생 시 해당 문자열을 제거해줌\n",
    "#     # df = df.applymap(lambda x: x.replace('오류난 문자열',''))  \n",
    "\n",
    "#     df.to_csv('./natepann.csv', mode = 'w', encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    url_list = Get_url()\n",
    "    content_list, reply_list, subreply_list = Crawling(url_list)\n",
    "#     Save_csv(content_list, reply_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
